{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    layer_dims -- neuro # in each layer\n",
    "    \n",
    "    return:\n",
    "    w and b for each layer\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A_prev -- A from previous layer. For the first layer it will be X\n",
    "    W -- current W\n",
    "    b -- current b\n",
    "    activation -- sigmoid or relu\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z)\n",
    "    \n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    X -- features\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    assert (AL.shape == (1, X.shape[1])) \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    dZ -- next layer's dZ\n",
    "    cache -- param from forward_propagation\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    dA -- current dA\n",
    "    cache -- param from forward propagation\n",
    "    activation --  \"sigmoid\" or \"relu\"\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Param:\n",
    "    AL -- predicted label\n",
    "    Y -- actual label\n",
    "    caches \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # layers of neural network\n",
    "    Y = Y.reshape(AL.shape) # match dimensions\n",
    "    \n",
    "    # da for last layer\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # dw and db for last layer\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(\n",
    "                                                                                            dAL, \n",
    "                                                                                            current_cache,\n",
    "                                                                                            activation = \"sigmoid\")\n",
    "\n",
    "    # calculate gradient for each layer before by using relu\n",
    "    for c in reversed(range(1,L)): \n",
    "        grads[\"dA\" + str(c-1)], grads[\"dW\" + str(c)], grads[\"db\" + str(c)] = linear_activation_backward(\n",
    "            grads[\"dA\" + str(c)], \n",
    "            caches[c-1],\n",
    "            activation = \"relu\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- w and b \n",
    "    grads -- gradient\n",
    "    learning_rate \n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # num of layers\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print (\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print (\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print (\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "m_train = train_x_orig.shape[0] # num of train data\n",
    "m_test = test_x_orig.shape[0] # num of test data\n",
    "num_px = test_x_orig.shape[1] # width and length for each pic\n",
    "\n",
    "# flatten and tranformation\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T \n",
    "\n",
    "# normalization\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning network model\n",
    "def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
    "    \"\"\"    \n",
    "    Param:\n",
    "    X -- data\n",
    "    Y -- label\n",
    "    layers_dims -- layers of network, num of neuros per layer\n",
    "    learning_rate \n",
    "    num_iterations \n",
    "    print_cost \n",
    "    \n",
    "    return:\n",
    "    parameters -- trained parameters for further optimization\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                  \n",
    "\n",
    "    # initialization\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # train data\n",
    "    for i in range(0, num_iterations):\n",
    "        # forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        # calculate cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "        # backward propagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        # optimization\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print (\"after %i training, the cost is: %f\" % (i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # draw the char\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "X = train_x\n",
    "Y = train_y\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "AL, caches = L_model_forward(X, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 100 training, the cost is: 0.672053\n",
      "after 200 training, the cost is: 0.648263\n",
      "after 300 training, the cost is: 0.611507\n",
      "after 400 training, the cost is: 0.567047\n",
      "after 500 training, the cost is: 0.540138\n",
      "after 600 training, the cost is: 0.527930\n",
      "after 700 training, the cost is: 0.465477\n",
      "after 800 training, the cost is: 0.369126\n",
      "after 900 training, the cost is: 0.391747\n",
      "after 1000 training, the cost is: 0.315187\n",
      "after 1100 training, the cost is: 0.272700\n",
      "after 1200 training, the cost is: 0.237419\n",
      "after 1300 training, the cost is: 0.199601\n",
      "after 1400 training, the cost is: 0.189263\n",
      "after 1500 training, the cost is: 0.161189\n",
      "after 1600 training, the cost is: 0.148214\n",
      "after 1700 training, the cost is: 0.137775\n",
      "after 1800 training, the cost is: 0.129740\n",
      "after 1900 training, the cost is: 0.121225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setup a neural network\n",
    "layers_dims = [12288, 20, 7, 5, 1]\n",
    "parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)\n",
    "# train_x shape: (12288, 209)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters):   \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # first propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "   \n",
    "    # normalization\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9808612440191385\n"
     ]
    }
   ],
   "source": [
    "# prediction of train data\n",
    "pred_train = predict(train_x,parameters)\n",
    "print(\"Accuracy: \"  + str(np.sum((pred_train == train_y) / train_x.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "# prediction of test data\n",
    "pred_test = predict(test_x,parameters)\n",
    "print(\"Accuracy: \"  + str(np.sum((pred_test == test_y) / test_x.shape[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
